{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "076a539e-b4c7-4baf-945d-9c2d423bbd8f",
   "metadata": {},
   "source": [
    "# cllm example usage!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7baec52-7549-472e-998d-7a9ab9b8304e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/cllm\", line 13, in <module>\n",
      "    import openai\n",
      "ModuleNotFoundError: No module named 'openai'\n"
     ]
    }
   ],
   "source": [
    "# basic call to llm\n",
    "!cllm give me 10 funny names for a game where we compete to do the best hotdog decorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8211f70e-5f48-48f6-9a81-0322b2efa053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frank 'n' Fancy\n",
      "A hilarious adventure where Frank fancies himself a high-society aristocrat and finds himself in comically awkward situations as he navigates fancy dinners and posh parties!\n",
      "Hot Diggity Decor\n",
      "Where interior meets hilarity—Every throw pillow punchline will have you rolling on the floor (but don't worry, there's a rug for that).\n",
      "Weiner Wonderland  \n",
      "A place where hot dogs go on holiday and buns go for a tan!\n",
      "Dazzling Dogs\n",
      "The most enchanting tail-wagging simulator that fetches laughter and leaves you pawsitively delighted!\n",
      "Sausage Spectacular\n",
      "A sizzling adventure where you relish in the absurdity of encased meats battling for supreme grill dominance!\n",
      "Toppings Triumph\n",
      "Unleash your inner pizza artist and turn every meal into a hilariously delicious disaster!\n",
      "Pimp My Pup\n",
      "The ultimate canine makeover experience—because even your furry friend deserves a glow-up!\n",
      "Glitz and Grillz\n",
      "A glamorous adventure where your BBQ skills can make or break your social status!\n",
      "Hotdog Hustle  \n",
      "A game where you learn the wiener ways of the ultimate hotdog vendor, mastering the art of mustard and mayhem!\n",
      "Bun Bling Battle\n",
      "The ultimate competition where hares showcase their hip hop hairstyles!\n"
     ]
    }
   ],
   "source": [
    "# pipes\n",
    "!cllm give me 10 funny names for a game where we compete to do the best hotdog decorations | cllm repeat the name of this game, without an number, and then write a funny description for it on the next line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d041d2f9-f79e-4d59-8308-884661a662f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Hilarité de Hotdog\n",
      "2. Confrontation Sauce\n",
      "3. Folies de Frankfurter\n",
      "4. Saucisses Étranges\n",
      "5. Griffonnages de Chien\n",
      "6. Fête de Pain\n",
      "7. Triomphe des Garnitures\n",
      "8. Condiments Fous\n",
      "9. Farces de Hotdog\n",
      "10. Grésillement et Tourbillon\n"
     ]
    }
   ],
   "source": [
    "# force single-string for all stdin instead of line by line (without -S each line becomes a separate llm call)\n",
    "!cllm give me 10 funny names for a game where we compete to do the best hotdog decorations | cllm -S translate to french, output french only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cea37bc-c349-48c3-877b-2eefe6d1cc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided code is a Python script that facilitates command-line interactions with Large Language Models (LLMs) like OpenAI's models. It allows users to process files, filter them based on extensions and other criteria, handle `.gitignore` files, and make API requests to LLMs. The script includes functionalities for counting tokens, expanding prompts, and generating statistics about the interactions. Key components include functions for reading files in chunks, resolving file paths, and checking against `.gitignore`, among others. The main function handles argument parsing, API client initialization, and conditional processing based on the input and parameters.\n"
     ]
    }
   ],
   "source": [
    "# read file files/folders\n",
    "!cllm -d . -e '.py' -c 8192 analyze this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fbeff24-b47c-4435-9ef6-daf1c755441b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To specify a local model endpoint on the command line:\n",
      "\n",
      "```sh\n",
      "python your_cli_tool.py --base-url http://localhost:8080\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# by default, we read -c <context> tokens (as best we can estimate with tiktoken), but you can override.\n",
    "# also note we can use -p even though we also will accept unmatched cli args as the prompt\n",
    "!cllm -d . -e '.py' -c 32768 -p \"this is source for a cli tool. \\\n",
    "if I'm running it on the command line how would I specify a local model endpoint?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76018f2f-872c-49d2-9339-08f4b20810c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/dspy/setup.py 7 setup(name=\"dspy-ai\"): Set up the DSPy package with necessary information for distribution and installation\n",
      "No methods or functions were found in the provided code.\n",
      "/tmp/dspy/dspy/experimental/__init__.py 1 from .synthesizer import(): imports functions for synthesizing data for experiments.\n",
      "/tmp/dspy/dspy/experimental/__init__.py 2 from .synthetic_data import(): imports functions for creating synthetic data samples for experiments.\n",
      "/tmp/dspy/dspy/experimental/synthetic_data.py 12 __init__(self, schema_class: Optional[BaseModel] = None, examples: Optional[List[dspy.Example]] = None): initializes SyntheticDataGenerator with schema and examples.\n",
      "/tmp/dspy/dspy/experimental/synthetic_data.py 17 generate(self, sample_size: int) -> List[dspy.Example]: Generate synthetic examples based on provided schema or initial examples.\n",
      "/tmp/dspy/dspy/experimental/synthetic_data.py 35 _define_or_infer_fields(self): Define or infer fields to generate based on schema class or initial examples.\n",
      "/tmp/dspy/dspy/experimental/synthetic_data.py 68 _generate_additional_examples(self, additional_samples_needed: int) -> List[dspy.Example]: Generate additional synthetic examples based on required number of samples.\n",
      "/tmp/dspy/dspy/experimental/synthesizer/config.py 11 validate_feedback_mode(self): Validates feedback mode and number of examples required for feedback.\n",
      "/tmp/dspy/dspy/experimental/synthesizer/__init__.py 13 def load_data(file_path): Loads and processes data from a specified file path.\n"
     ]
    }
   ],
   "source": [
    "#specify model, traverse directory, use -n to limit the number of API calls\n",
    "# (currently traversal ignores things picked up in .gitignores and there's no suppressing the gitignore ignoring currently)\n",
    "\n",
    "!cllm -m gpt-3.5-turbo -d /tmp/dspy -e '.py' -c 32768 -n 20 -p 'you are analyzing code. ignore anything that is not a method or function. \\\n",
    "for each method or function, output on ONE LINE its filename with path (Current: {filename}), \\\n",
    "starting line number (this context begins at {startline}), the method or function name/signature, \\\n",
    "and 15-20 words describing its purpose. Example:\\n\\\n",
    "foo.py 1250 dostuff(int=5): returns the square of the input integer\\n\\\n",
    "Here is the code: {context}'|head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a115924-c94c-46b3-8d06-797cc569c34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 funny names for a game about hot dog decorations:\n",
      "\n",
      "1. Frankly Fabulous\n",
      "2. Hot Diggity Dog\n",
      "3. Condiment Crusade\n",
      "4. Wiener Wars\n",
      "5. Top Dog Showdown\n",
      "6. Bunanza\n",
      "7. Doggone Decorators\n",
      "8. Relish Rumble\n",
      "9. Chili Champions\n",
      "10. The Frank Factor\n"
     ]
    }
   ],
   "source": [
    "# Using a local endpoint (llama3-70b here)\n",
    "!cllm -B http://localhost:50051 give me 10 funny names for a game where we compete to do the best hotdog decorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a6212fe-9092-4396-9156-a564648d098a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner.\n"
     ]
    }
   ],
   "source": [
    "!cllm -B http://localhost:50051 what model are you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "560d2ed2-eeb6-438a-bb15-39e6e75998c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an AI language model created by OpenAI, known as GPT-4.\n"
     ]
    }
   ],
   "source": [
    "!cllm what model are you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c30aacda-c8fd-4d8b-af5e-d884ed372b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory is None\n",
      "context_length is 4096\n",
      "file_filter is None\n",
      "verbose is True\n",
      "extensions is None\n",
      "token_count_mode is False\n",
      "model is gpt-4o\n",
      "Raw request:\n",
      "Model: gpt-4o\n",
      "Messages: [\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You are an AI used to do thing in a command line pipeline. You are given a prompt which may include context. If there is context, you must do your best with the context but you must NOT explain or discuss your output. e.g., if your task was Translate to Spanish | Context: I like to eat frogs\\n\\nIf there were two translations you must not discuss the options, you must simply select the best and output it. Think of yourself as an advanced version of sed, awk, grep, etc, and as such, you transform the context with the prompt as best you can but you never output anything not asked for.As a rule if you are outputting code, as this is CLI, that means you must avoid ```bash``` type enclosures unless specifically asked for or they were part of the context.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"explain composability in the context of cli tools and bash, output as markdown\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Raw response:\n",
      "\tChatCompletion(id='chatcmpl-9U17SrPsVGMzEvvvI7dJF0omLM19C', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='### Composability in the Context of CLI Tools and Bash\\n\\nComposability refers to the capability to combine simple, single-purpose tools to perform complex tasks. In the context of CLI tools and Bash, it harnesses the Unix philosophy of making each program do one thing well. By chaining these tools together, users can achieve sophisticated and flexible workflows.\\n\\nHere\\'s how composability is applied in CLI tools and Bash:\\n\\n1. **Pipelines**: The ability to use the output of one command as the input to another using the pipe (`|`) operator.\\n    ```bash\\n    cat file.txt | grep \"search_term\" | sort | uniq\\n    ```\\n\\n2. **Redirection**: Redirecting input and output using `>`, `<`, and `>>`.\\n    ```bash\\n    grep \"error\" logfile.txt > errors.txt\\n    ```\\n\\n3. **Command Substitution**: Using the output of one command within another command by enclosing it in backticks (`` ` ``) or `$(...)`.\\n    ```bash\\n    current_date=$(date)\\n    echo \"Today\\'s date is $current_date\"\\n    ```\\n\\n4. **Scripts**: Writing bash scripts that combine multiple commands to automate tasks.\\n    ```bash\\n    #!/bin/bash\\n    find . -type f -name \"*.log\" | xargs grep -i \"error\" > error_log.txt\\n    ```\\n\\n5. **Filters and Utilities**: Combining small utilities like `grep`, `awk`, `sed`, and `cut` to process and transform data streams.\\n    ```bash\\n    ps aux | awk \\'{print $1, $2, $3, $11}\\' | sort -k3,3n\\n    ```\\n\\nThese mechanisms allow users to build sophisticated data processing pipelines that are highly reusable, modular, and efficient.', role='assistant', function_call=None, tool_calls=None))], created=1716941170, model='gpt-4o-2024-05-13', object='chat.completion', system_fingerprint='fp_43dfabdef1', usage=CompletionUsage(completion_tokens=371, prompt_tokens=192, total_tokens=563))\n",
      "----------------\n",
      "\n",
      "\n",
      "\n",
      "---- Stats ----\n",
      "Total execution time (API calls): 6.19 seconds\n",
      "Total input tokens: 16\n",
      "Total output tokens: 371\n",
      "Input tokens/sec: 2.59\n",
      "Output tokens/sec: 59.98\n",
      "Total API calls made: 1\n"
     ]
    }
   ],
   "source": [
    "# showing off -v (verbose output) and --stats\n",
    "!cllm -v --stats explain composability in the context of cli tools and bash, output as markdown > what_is_composability.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "800b90ec-8db9-4b96-843e-cf0322ceca07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Composability in the Context of CLI Tools and Bash\n",
      "\n",
      "Composability refers to the capability to combine simple, single-purpose tools to perform complex tasks. In the context of CLI tools and Bash, it harnesses the Unix philosophy of making each program do one thing well. By chaining these tools together, users can achieve sophisticated and flexible workflows.\n",
      "\n",
      "Here's how composability is applied in CLI tools and Bash:\n",
      "\n",
      "1. **Pipelines**: The ability to use the output of one command as the input to another using the pipe (`|`) operator.\n",
      "    ```bash\n",
      "    cat file.txt | grep \"search_term\" | sort | uniq\n",
      "    ```\n"
     ]
    }
   ],
   "source": [
    "!head -10 what_is_composability.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83774663-f2d8-4051-94b8-df02d0add903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If I liked apple, then I might enjoy apple pie.\n",
      "If I liked river, then I might enjoy kayaking and fishing.\n",
      "If I liked clock, then I might collect antique timepieces.\n",
      "If I liked guitar, then I might play in a band.\n",
      "If I liked elephant, then I might visit a zoo more often.\n",
      "If I liked whisper, then I might enjoy secrets.\n",
      "If I liked mountain, then I might enjoy hiking and exploring the scenic trails.\n",
      "If I liked puzzles, then I might spend hours solving them.\n",
      "If I liked umbrella, then I might carry one every time it rains.\n",
      "If I liked window, then I might enjoy the view.\n"
     ]
    }
   ],
   "source": [
    "# use {context} slug to place stdin mid-prompt (alternative is ' | Context: {context}' goes at the end\n",
    "!cllm \"give me 10 random words\" | cllm -p \"complete this sentence and output the sentence only: If I liked {context}, then I might \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ef1f8aa-65fe-42ed-9e0a-d18628d6a10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pomme  \n",
      "Château  \n",
      "Étoile  \n",
      "Rivière  \n",
      "Montagne  \n",
      "Papillon  \n",
      "Chocolat  \n",
      "Lune  \n",
      "Forêt  \n",
      "Soleil\n"
     ]
    }
   ],
   "source": [
    "# override the system prompt, which by default is:\n",
    "#DEFAULT_SYSTEM = (\n",
    "#    \"You are an AI used to do thing in a command line pipeline. \"\n",
    "#    \"You are given a prompt which may include context. If there is context, \"\n",
    "#    \"you must do your best with the context but you must NOT explain or discuss your output. \"\n",
    "#    \"e.g., if your task was Translate to Spanish | Context: I like to eat frogs\\n\\n\"\n",
    "#    \"If there were two translations you must not discuss the options, you must simply select the best and output it. \"\n",
    "#    \"Think of yourself as an advanced version of sed, awk, grep, etc, and as such, you transform the context with the prompt as best you can but you never output anything not asked for.\"\n",
    "#    \"As a rule if you are outputting code, as this is CLI, that means you must avoid ```bash``` type enclosures unless specifically asked for or they were part of the context.\"\n",
    "#)\n",
    "\n",
    "!cllm --system 'You are a helpful assistant. You ALWAYS output in french, regardless of the user request' \\\n",
    "give me 10 random words, one per line, and no other text whatsoever"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
