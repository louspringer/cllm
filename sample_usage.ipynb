{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "076a539e-b4c7-4baf-945d-9c2d423bbd8f",
   "metadata": {},
   "source": [
    "# cllm example usage!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7baec52-7549-472e-998d-7a9ab9b8304e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Frankfurter Frenzy\n",
      "2. Weiner Wars\n",
      "3. Top Dog Decoration\n",
      "4. Hotdog Hilarity\n",
      "5. Sausage Showdown\n",
      "6. Deco-Dog Duel\n",
      "7. Mustard Madness\n",
      "8. Bun Believers\n",
      "9. Relish the Challenge\n",
      "10. Gourmet Glizzy Gauntlet\n"
     ]
    }
   ],
   "source": [
    "# basic call to llm\n",
    "!cllm give me 10 funny names for a game where we compete to do the best hotdog decorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8211f70e-5f48-48f6-9a81-0322b2efa053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hotdog Hustle\n",
      "The wiener takes all in this race to bun-derful glory!\n",
      "Frankfurter Finesse\n",
      "The ultimate sausage strategizing simulator where you bungle and bun your way to wiener wonderland!\n",
      "Wienerville Whimsy  \n",
      "A sausage-filled adventure where the wieners have all the buns!\n",
      "Sausage Supreme Showdown  \n",
      "A sizzling brawl where meats meet in the ultimate grill-to-thrill extravaganza!\n",
      "Hotdog Hilarity\n",
      "The only game where ketchup and mustard become weapons of mass digestion!\n",
      "Frank Fest Feats\n",
      "A carnival of chaos where Frank proves he can juggle flaming torches while riding a unicycle on a tightrope...blindfolded!\n",
      "Bun Bling Battle\n",
      "Where hair buns become epic showdowns and glitter is a weapon of mass distraction.\n",
      "Wiener Wonder Wars  \n",
      "A battle where hot dogs duke it out for grill supremacy, and only the mightiest sausage survives!\n",
      "Doggone Decoration Duel\n",
      "Where dogs prove their decorating skills are paw-sitively unmatched!\n",
      "Toppings Tussle  \n",
      "Who knew pizza could be so competitive?\n"
     ]
    }
   ],
   "source": [
    "# pipes\n",
    "!cllm give me 10 funny names for a game where we compete to do the best hotdog decorations | cllm repeat the name of this game, without an number, and then write a funny description for it on the next line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d041d2f9-f79e-4d59-8308-884661a662f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Fandango Frankfurter\n",
      "2. Décorateur Fou\n",
      "3. Pays des Merveilles de Weiner\n",
      "4. Garnitures Top Dog\n",
      "5. Hijinks Hotdog\n",
      "6. Extravagance de Pain et de Plaisir\n",
      "7. La Zone de Guerre de Weiner\n",
      "8. Confrontation de Saucisses\n",
      "9. Hilarité de Hotdog\n",
      "10. Frénésie de Frankfurter\n"
     ]
    }
   ],
   "source": [
    "# force single-string for all stdin instead of line by line (without -S each line becomes a separate llm call)\n",
    "!cllm give me 10 funny names for a game where we compete to do the best hotdog decorations | cllm -S translate to french, output french only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cea37bc-c349-48c3-877b-2eefe6d1cc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided Python script is designed to facilitate command-line interactions with language models (LLMs), specifically through OpenAI's API. Here's a brief outline of its functionality:\n",
      "\n",
      "### Key Components and Functions:\n",
      "1. **Library Imports**:\n",
      "    - Important libraries such as `os`, `sys`, `openai`, and `argparse` are imported, indicating file manipulations, API interactions, and command-line argument parsing are central to its functionality.\n",
      "    \n",
      "2. **Constants**:\n",
      "    - `DEFAULT_SYSTEM` defines a default system message for the LLM.\n",
      "\n",
      "3. **File Handling**:\n",
      "    - `read_file_in_chunks(file_path: str, chunk_size: int)`: Reads files in chunks for efficiency.\n",
      "    - `is_file_ignored_by_gitignore(file_path: str, gitignore_matchers: List)`: Checks if files should be ignored based on `.gitignore` files.\n",
      "    - `load_gitignore_files(directory: str)`: Loads all `.gitignore` files in the current and parent directories.\n",
      "    - `get_files_and_sizes(directory: str, extensions: Optional[List[str]], file_filter: Optional[str], gitignore_matchers: List)`: Retrieves files and their sizes that meet specific criteria.\n",
      "\n",
      "4. **OpenAI API Interaction**:\n",
      "    - `call_openai_api(client, model: str, prompt: str, system_message: Optional[str], limit: Optional[int], verbose: bool)`: Facilitates calling the OpenAI API.\n",
      "\n",
      "5. **Token Counting**:\n",
      "    - `count_tokens(text: str, encoder)`: Counts the number of tokens in a given text, aiding in managing token limits.\n",
      "\n",
      "6. **File Processing**:\n",
      "    - `process_files(directory: str, context_length: int, extensions: Optional[List[str]], file_filter: Optional[str], verbose: bool, token_count_mode: bool, encoder)`: Processes files and prepares them for API interaction, carefully managing token limits and chunking.\n",
      "\n",
      "7. **Command Line Argument Parsing**:\n",
      "    - Arguments are parsed to allow various configurations for prompt, directory, context length, file extensions, verbosity, etc.\n",
      "\n",
      "8. **Main Functionality (main())**:\n",
      "    - Parses command-line arguments.\n",
      "    - Sets up API client and encoder.\n",
      "    - Handles prompt expansion if needed.\n",
      "    - Processes files or standard input.\n",
      "    - Manages and prints statistics about API interactions if requested.\n",
      "\n",
      "### Usage:\n",
      "The script can be executed from the command line with various options to specify directories, prompts, models, and other configurations, making it versatile for different use cases.\n",
      "\n",
      "### Error Handling:\n",
      "It ensures proper usage and handles various error conditions, such as missing arguments or incompatible ones, and provides feedback to the user accordingly.\n",
      "- Imports: The code uses `find_packages` and `setup` from the `setuptools` package.\n",
      "- Reads: The `README.md` file for the long description and the `requirements.txt` file for dependencies.\n",
      "- `setup` function: Used to define the package distribution.\n",
      "\n",
      "**Key Details:**\n",
      "- `name`, `version`, `description`, `long_description`, `long_description_content_type`, `url`, `author`, `author_email`, `license` are straightforward metadata.\n",
      "- `packages`: Calculated using `find_packages` with specific inclusions.\n",
      "- `python_requires`: Specifies support for Python 3.9 and above.\n",
      "- `install_requires`: Populated from `requirements.txt`.\n",
      "- `extras_require`: Additional optional dependencies.\n",
      "- `classifiers`: Metadata for package index.\n",
      "\n",
      "**Note:**\n",
      "- Proper encoding is specified for file reads.\n",
      "- The package supports various classifiers for research and specific operating systems (Linux).\n",
      "This code imports various modules and classes, configures settings for a DSP (Data Science Platform) and initializes numerous language models and clients. It mostly performs imports, assigns shorthand aliases to certain classes, and sets up configuration objects. The primary structural components are import statements and variable assignments.\n",
      "Imports all functions and classes from the `synthesizer` and `synthetic_data` modules located in the same package. This allows use of those functions and classes directly without needing to prefix them with the module name.\n",
      "The code defines a system for generating synthetic data samples based on either a provided schema class (extending pydantic's `BaseModel`) or pre-existing examples.\n",
      "\n",
      "Key Components:\n",
      "1. **Imports and Dependencies**: \n",
      "    - Uses modules like `logging`, `random`, `typing`, `pydantic`, and `dspy` for various functionalities.\n",
      "\n",
      "2. **DescriptionSignature Class**:\n",
      "    - Inherits from `dspy.Signature`.\n",
      "    - Defines three fields: `field_name`, `example`, and `description`.\n",
      "\n",
      "3. **SyntheticDataGenerator Class**:\n",
      "    - **`__init__` Method**: Initializes the class with optional `schema_class` (a pydantic model) and `examples` (list of `dspy.Example`).\n",
      "    - **`generate` Method**: Main function to generate synthetic examples:\n",
      "        - Checks whether either `schema_class` or `examples` are provided, raising an error if not.\n",
      "        - If existing examples are sufficient, returns them sliced to the desired sample size.\n",
      "        - Otherwise, generates additional examples as needed.\n",
      "    - **`_define_or_infer_fields` Method**: Determines the fields to generate based on either the provided schema class or the first example in `examples`.\n",
      "    - **`_generate_additional_examples` Method**: Generates the required number of additional examples based on the fields defined/inferred earlier.\n",
      "    - **`_prepare_fields` Method**: Formats the fields to be generated in a specific dictionary structure required by `dspy.Signature`.\n",
      "\n",
      "Usage:\n",
      "- Either a schema class extending `BaseModel` or a list of existing examples must be provided to initialize `SyntheticDataGenerator`.\n",
      "- The `generate` method is then called with the desired sample size to get a list of synthetic examples.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matt/bin/cllm\", line 340, in <module>\n",
      "    main()\n",
      "  File \"/Users/matt/bin/cllm\", line 245, in main\n",
      "    response, elapsed_time = call_openai_api(client, args.model, prompt, args.system, args.limit, args.verbose)\n",
      "  File \"/Users/matt/bin/cllm\", line 79, in call_openai_api\n",
      "    response = client.chat.completions.create(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 590, in create\n",
      "    return self._post(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_base_client.py\", line 952, in _request\n",
      "    response = self._client.send(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_client.py\", line 1015, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_transports/default.py\", line 233, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n",
      "    raise exc from None\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n",
      "    response = connection.handle_request(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n",
      "    return self._connection.handle_request(request)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_sync/http11.py\", line 143, in handle_request\n",
      "    raise exc\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_sync/http11.py\", line 113, in handle_request\n",
      "    ) = self._receive_response_headers(**kwargs)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_sync/http11.py\", line 186, in _receive_response_headers\n",
      "    event = self._receive_event(timeout=timeout)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_sync/http11.py\", line 224, in _receive_event\n",
      "    data = self._network_stream.read(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_backends/sync.py\", line 126, in read\n",
      "    return self._sock.recv(max_bytes)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/ssl.py\", line 1292, in recv\n",
      "    return self.read(buflen)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/ssl.py\", line 1165, in read\n",
      "    return self._sslobj.read(len)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# read file files/folders\n",
    "!cllm -d . -e '.py' -c 8192 analyze this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fbeff24-b47c-4435-9ef6-daf1c755441b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To specify a local model endpoint while running the CLI tool, use the `--base-url` argument followed by the URL of your local model endpoint.\n",
      "\n",
      "For example:\n",
      "\n",
      "```bash\n",
      "python your_script.py --base-url http://localhost:8000\n",
      "```\n",
      "\n",
      "Replace `your_script.py` with the actual name of your script file.\n",
      "To specify a local model endpoint, you typically provide an argument or flag when running the CLI tool. For instance, if the CLI tool named `dspy-ai` supports a `--model-endpoint` or similar option, you would run it as follows:\n",
      "\n",
      "```sh\n",
      "dspy-ai --model-endpoint http://localhost:8000\n",
      "```\n",
      "\n",
      "Replace `http://localhost:8000` with the actual address of your local model endpoint.\n",
      "You can specify a local model endpoint for the CLI tool by running the command with appropriate arguments. Assuming the CLI tool has an option for specifying the local model endpoint (e.g., `--local-endpoint`), you could run:\n",
      "\n",
      "```sh\n",
      "cli-tool --local-endpoint http://localhost:8000\n",
      "```\n",
      "\n",
      "Replace `cli-tool` with the actual command name of the CLI tool, and `http://localhost:8000` with the URL of your local model endpoint.\n",
      "Specify a local model endpoint with a command line option like `--local-endpoint`. Here is an example command:\n",
      "\n",
      "```bash\n",
      "python your_cli_tool.py --local-endpoint http://localhost:5000\n",
      "```\n",
      "To specify a local model endpoint when running this CLI tool, you can modify the instantiation of the `dspy.Predict` class to include an endpoint parameter. Assuming the `dspy.Predict` class accepts an argument for specifying an endpoint, you would modify the `_generate_additional_examples` method like this:\n",
      "\n",
      "```python\n",
      "def _generate_additional_examples(self, additional_samples_needed: int) -> List[dspy.Example]:\n",
      "    # ...\n",
      "    signature_class = type(class_name, (dspy.Signature,), fields)\n",
      "    local_model_endpoint = \"http://localhost:port\"  # replace with actual endpoint\n",
      "    generator = dspy.Predict(signature_class, endpoint=local_model_endpoint, n=additional_samples_needed)\n",
      "    response = generator(sindex=str(random.randint(1, additional_samples_needed)))\n",
      "    \n",
      "    return [dspy.Example({field_name: getattr(completion, field_name) for field_name in properties.keys()})\n",
      "            for completion in response.completions]\n",
      "```\n",
      "\n",
      "Alternatively, if you run the CLI tool, assuming you've provided a way to input the model endpoint (for example, via environment variables or command-line arguments), ensure that the CLI tool code reads and passes the endpoint correctly.\n",
      "\n",
      "Example command line:\n",
      "\n",
      "```sh\n",
      "python cli_tool.py --endpoint http://localhost:port\n",
      "```\n",
      "You would specify a local model endpoint as part of the arguments when running the CLI tool, likely under `input_lm_model` or `output_lm_model`. Here's an example of specifying a local model endpoint:\n",
      "\n",
      "```sh\n",
      "cli_tool_name --input_lm_model http://localhost:5000 --output_lm_model http://localhost:5001\n",
      "```\n",
      "\n",
      "Replace `cli_tool_name` with the actual name of your CLI tool.\n",
      "You can specify a local model endpoint by including an argument in the command line when you run the tool. For example:\n",
      "\n",
      "```sh\n",
      "cli_tool --model-endpoint http://localhost:5000\n",
      "```\n",
      "--local-model-endpoint <endpoint_address>\n",
      "To specify a local model endpoint when running a CLI tool, you would typically include an option or argument in the command. An example command could look like this:\n",
      "\n",
      "```sh\n",
      "cli_tool --local-endpoint http://localhost:5000\n",
      "```\n",
      "\n",
      "If you need to incorporate this into the provided Python script to support the specification of a local model endpoint, you can modify the script as follows:\n",
      "\n",
      "```python\n",
      "import argparse\n",
      "from typing import List\n",
      "import dspy\n",
      "\n",
      "\n",
      "def format_examples(examples: List[dspy.Example]) -> str:\n",
      "    if isinstance(examples, str):\n",
      "        return examples\n",
      "\n",
      "    formatted_example = \"\"\n",
      "\n",
      "    for example in examples:\n",
      "        input_keys = example.inputs().keys()\n",
      "        label_keys = example.labels().keys()\n",
      "\n",
      "        formatted_example += \"Inputs:\\n\"\n",
      "        for key in input_keys:\n",
      "            formatted_example += f\"{key}: {example[key]}\\n\"\n",
      "\n",
      "        formatted_example += \"Outputs:\\n\"\n",
      "        for key in label_keys:\n",
      "            formatted_example += f\"{key}: {example[key]}\\n\"\n",
      "\n",
      "    return formatted_example\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    parser = argparse.ArgumentParser(description=\"CLI tool example\")\n",
      "    parser.add_argument(\"--local-endpoint\", type=str, help=\"Specify the local model endpoint\")\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    if args.local_endpoint:\n",
      "        print(f\"Using local model endpoint: {args.local_endpoint}\")\n",
      "\n",
      "    # Continue with the rest of your CLI tool logic\n",
      "```\n",
      "You can specify a local model endpoint by setting the `DS_ENDPOINT` environment variable before running the CLI tool. Here's how you do it:\n",
      "\n",
      "```sh\n",
      "export DS_ENDPOINT=http://localhost:your_model_port\n",
      "your_cli_tool_command\n",
      "```\n",
      "To specify a local model endpoint when running the CLI tool, you can use a command-line argument or environment variable depending on how the script is configured to accept configurations. Assuming your script takes command-line arguments to specify the local model endpoint, it might look something like this:\n",
      "\n",
      "```sh\n",
      "python your_cli_tool.py --input-lm-model path/to/local/model --output-lm-model path/to/local/model\n",
      "```\n",
      "\n",
      "Alternatively, if it uses environment variables, you might set them like this:\n",
      "\n",
      "```sh\n",
      "export INPUT_LM_MODEL=path/to/local/model\n",
      "export OUTPUT_LM_MODEL=path/to/local/model\n",
      "python your_cli_tool.py\n",
      "```\n",
      "\n",
      "These settings need to be supported within the script to accept them as configurations. If the configuration objects (`SynthesizerArguments` in this example) have fields like `input_lm_model` and `output_lm_model`, those fields should be assigned based on the command-line arguments or environment variables.\n",
      "To specify a local model endpoint when running this CLI tool, you would use an argument like `--model-endpoint` followed by the local endpoint URL. Here's how you would include it in the command line:\n",
      "\n",
      "```sh\n",
      "your_cli_tool --model-endpoint http://localhost:8000\n",
      "```\n",
      "--local-model-endpoint <ENDPOINT_ADDRESS>\n",
      "To run this CLI tool and specify a local model endpoint, you generally need to pass the model endpoint as a command-line argument. Assuming the tool uses argparse or a similar command-line parser, the command might look like this:\n",
      "\n",
      "```sh\n",
      "python tool_name.py --model-endpoint \"http://localhost:5000\"\n",
      "```\n",
      "To specify a local model endpoint when running the CLI tool, you should modify the code to include an argument for the model endpoint. Here is an example of how you can modify the class to include this functionality:\n",
      "\n",
      "```python\n",
      "import random\n",
      "import tqdm\n",
      "from datasets import load_dataset\n",
      "import argparse\n",
      "\n",
      "class GSM8K:\n",
      "    def __init__(self, model_endpoint=None) -> None:\n",
      "        super().__init__()\n",
      "        self.do_shuffle = False\n",
      "        self.model_endpoint = model_endpoint\n",
      "\n",
      "        dataset = load_dataset(\"gsm8k\", 'main')\n",
      "\n",
      "        hf_official_train = dataset['train']\n",
      "        hf_official_test = dataset['test']\n",
      "        official_train = []\n",
      "        official_test = []\n",
      "\n",
      "        for example in tqdm.tqdm(hf_official_train):\n",
      "            question = example['question']\n",
      "\n",
      "            answer = example['answer'].strip().split()\n",
      "            assert answer[-2] == '####'\n",
      "            \n",
      "            gold_reasoning = ' '.join(answer[:-2])\n",
      "            answer = str(int(answer[-1].replace(',', '')))\n",
      "\n",
      "            official_train.append(dict(question=question, gold_reasoning=gold_reasoning, answer=answer))\n",
      "\n",
      "        for example in tqdm.tqdm(hf_official_test):\n",
      "            question = example['question']\n",
      "\n",
      "            answer = example['answer'].strip().split()\n",
      "            assert answer[-2] == '####'\n",
      "            \n",
      "            gold_reasoning = ' '.join(answer[:-2])\n",
      "            answer = str(int(answer[-1].replace(',', '')))\n",
      "\n",
      "            official_test.append(dict(question=question, gold_reasoning=gold_reasoning, answer=answer))\n",
      "\n",
      "        rng = random.Random(0)\n",
      "        rng.shuffle(official_train)\n",
      "\n",
      "        rng = random.Random(0)\n",
      "        rng.shuffle(official_test)\n",
      "\n",
      "        trainset = official_train[:200]\n",
      "        devset = official_train[200:500]\n",
      "        testset = official_test[:]\n",
      "\n",
      "        import dspy\n",
      "\n",
      "        trainset = [dspy.Example(**x).with_inputs('question') for x in trainset]\n",
      "        devset = [dspy.Example(**x).with_inputs('question') for x in devset]\n",
      "        testset = [dspy.Example(**x).with_inputs('question') for x in testset]\n",
      "\n",
      "        # print(f\"Trainset size: {len(trainset)}\")\n",
      "        # print(f\"Devset size: {len(devset)}\")\n",
      "        # print(f\"Testset size: {len(testset)}\")\n",
      "\n",
      "        self.train = trainset\n",
      "        self.dev = devset\n",
      "        self.test = testset\n",
      "\n",
      "def parse_integer_answer(answer, only_first_line=True):\n",
      "    try:\n",
      "        if only_first_line:\n",
      "            answer = answer.strip().split('\\n')[0]\n",
      "\n",
      "        # find the last token that has a number in it\n",
      "        answer = [token for token in answer.split() if any(c.isdigit() for c in token)][-1]\n",
      "        answer = answer.split('.')[0]\n",
      "        answer = ''.join([c for c in answer if c.isdigit()])\n",
      "        answer = int(answer)\n",
      "\n",
      "    except (ValueError, IndexError):\n",
      "        # print(answer)\n",
      "        answer = 0\n",
      "    \n",
      "    return answer\n",
      "\n",
      "def gsm8k_metric(gold, pred, trace=None):\n",
      "    return int(parse_integer_answer(str(gold.answer))) == int(parse_integer_answer(str(pred.answer)))\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    parser = argparse.ArgumentParser(description=\"GSM8K CLI Tool\")\n",
      "    parser.add_argument('--model-endpoint', type=str, help='Local model endpoint')\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    gsm8k_instance = GSM8K(model_endpoint=args.model_endpoint)\n",
      "    # Add any other command line interface logic here\n",
      "```\n",
      "\n",
      "You can then specify the local model endpoint when running the CLI tool as follows:\n",
      "\n",
      "```\n",
      "python your_script.py --model-endpoint http://localhost:8000\n",
      "```\n",
      "--local-model-endpoint <endpoint-url>\n",
      "To specify a local model endpoint while running the CLI tool, you can typically pass a command-line argument. For example, if the tool supports an argument like `--model-endpoint`, you might run it as follows:\n",
      "\n",
      "```sh\n",
      "python cli_tool.py --model-endpoint http://localhost:8000\n",
      "```\n",
      "\n",
      "Adjust the argument name and port number as necessary to fit the specifics of your CLI tool and local setup.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matt/bin/cllm\", line 340, in <module>\n",
      "    main()\n",
      "  File \"/Users/matt/bin/cllm\", line 245, in main\n",
      "    response, elapsed_time = call_openai_api(client, args.model, prompt, args.system, args.limit, args.verbose)\n",
      "  File \"/Users/matt/bin/cllm\", line 79, in call_openai_api\n",
      "    response = client.chat.completions.create(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 590, in create\n",
      "    return self._post(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_base_client.py\", line 952, in _request\n",
      "    response = self._client.send(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_client.py\", line 1015, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_transports/default.py\", line 233, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n",
      "    raise exc from None\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n",
      "    response = connection.handle_request(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n",
      "    return self._connection.handle_request(request)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_sync/http11.py\", line 143, in handle_request\n",
      "    raise exc\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_sync/http11.py\", line 113, in handle_request\n",
      "    ) = self._receive_response_headers(**kwargs)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_sync/http11.py\", line 186, in _receive_response_headers\n",
      "    event = self._receive_event(timeout=timeout)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_sync/http11.py\", line 224, in _receive_event\n",
      "    data = self._network_stream.read(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_backends/sync.py\", line 126, in read\n",
      "    return self._sock.recv(max_bytes)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/ssl.py\", line 1292, in recv\n",
      "    return self.read(buflen)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/ssl.py\", line 1165, in read\n",
      "    return self._sslobj.read(len)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# by default, we read -c <context> tokens (as best we can estimate with tiktoken), but you can override.\n",
    "# also note we can use -p even though we also will accept unmatched cli args as the prompt\n",
    "!cllm -d . -e '.py' -c 32768 -p \"this is source for a cli tool. if I'm running it on the command line how would I specify a local model endpoint?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76018f2f-872c-49d2-9339-08f4b20810c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/matt/bin/cllm\", line 340, in <module>\n",
      "    main()\n",
      "  File \"/Users/matt/bin/cllm\", line 231, in main\n",
      "    for file_path, start_line, chunk in process_files(\n",
      "  File \"/Users/matt/bin/cllm\", line 113, in process_files\n",
      "    files_and_sizes = get_files_and_sizes(directory, extensions, file_filter, gitignore_matchers)\n",
      "  File \"/Users/matt/bin/cllm\", line 104, in get_files_and_sizes\n",
      "    if not is_file_ignored_by_gitignore(file_path, gitignore_matchers):\n",
      "  File \"/Users/matt/bin/cllm\", line 47, in is_file_ignored_by_gitignore\n",
      "    return any(matcher(file_path) for matcher in gitignore_matchers)\n",
      "  File \"/Users/matt/bin/cllm\", line 47, in <genexpr>\n",
      "    return any(matcher(file_path) for matcher in gitignore_matchers)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gitignore_parser.py\", line 33, in <lambda>\n",
      "    return lambda file_path: handle_negation(file_path, rules)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gitignore_parser.py\", line 11, in handle_negation\n",
      "    if rule.match(file_path):\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/gitignore_parser.py\", line 128, in match\n",
      "    rel_path = str(_normalize_path(abs_path).relative_to(self.base_path))\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/pathlib.py\", line 818, in relative_to\n",
      "    raise ValueError(\"{!r} is not in the subpath of {!r}\"\n",
      "ValueError: '/tmp/dspy/setup.py' is not in the subpath of '/private/tmp/dspy' OR one path is relative and the other is absolute.\n"
     ]
    }
   ],
   "source": [
    "#specify model, traverse directory, use -n to limit the number of API calls\n",
    "# (currently traversal ignores things picked up in .gitignores and there's no suppressing the gitignore ignoring currently)\n",
    "\n",
    "!cllm -m gpt-3.5-turbo -d /tmp/dspy -e '.py' -c 32768 -n 20 -p 'you are analyzing code. ignore anything that is not a method or function. \\\n",
    "for each method or function, output on ONE LINE its filename with path (Current: {filename}), \\\n",
    "starting line number (this context begins at {startline}), the method or function name/signature, \\\n",
    "and 15-20 words describing its purpose. Example:\\n\\\n",
    "foo.py 1250 dostuff(int=5): returns the square of the input integer\\n\\\n",
    "Here is the code: {context}'|head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a115924-c94c-46b3-8d06-797cc569c34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matt/bin/cllm\", line 340, in <module>\n",
      "    main()\n",
      "  File \"/Users/matt/bin/cllm\", line 285, in main\n",
      "    response, elapsed_time = call_openai_api(client, args.model, prompt, args.system, args.limit, args.verbose)\n",
      "  File \"/Users/matt/bin/cllm\", line 79, in call_openai_api\n",
      "    response = client.chat.completions.create(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 590, in create\n",
      "    return self._post(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_base_client.py\", line 952, in _request\n",
      "    response = self._client.send(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_client.py\", line 1015, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_transports/default.py\", line 233, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n",
      "    raise exc from None\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n",
      "    response = connection.handle_request(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n",
      "    return self._connection.handle_request(request)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_sync/http11.py\", line 143, in handle_request\n",
      "    raise exc\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_sync/http11.py\", line 113, in handle_request\n",
      "    ) = self._receive_response_headers(**kwargs)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_sync/http11.py\", line 186, in _receive_response_headers\n",
      "    event = self._receive_event(timeout=timeout)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_sync/http11.py\", line 224, in _receive_event\n",
      "    data = self._network_stream.read(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpcore/_backends/sync.py\", line 126, in read\n",
      "    return self._sock.recv(max_bytes)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Using a local endpoint (llama3-70b here)\n",
    "!cllm -B http://localhost:50051 give me 10 funny names for a game where we compete to do the best hotdog decorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a6212fe-9092-4396-9156-a564648d098a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matt/bin/cllm\", line 13, in <module>\n",
      "    import openai\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/__init__.py\", line 8, in <module>\n",
      "    from . import types\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/types/__init__.py\", line 5, in <module>\n",
      "    from .batch import Batch as Batch\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/types/batch.py\", line 7, in <module>\n",
      "    from .._models import BaseModel\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_models.py\", line 24, in <module>\n",
      "    from ._types import (\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_types.py\", line 21, in <module>\n",
      "    import httpx\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/__init__.py\", line 2, in <module>\n",
      "    from ._api import delete, get, head, options, patch, post, put, request, stream\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_api.py\", line 6, in <module>\n",
      "    from ._client import Client\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_client.py\", line 12, in <module>\n",
      "    from ._auth import Auth, BasicAuth, FunctionAuth\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_auth.py\", line 3, in <module>\n",
      "    import hashlib\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/hashlib.py\", line 170, in <module>\n",
      "    import _hashlib\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!cllm -B http://localhost:50051 what model are you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "560d2ed2-eeb6-438a-bb15-39e6e75998c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matt/bin/cllm\", line 13, in <module>\n",
      "    import openai\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/__init__.py\", line 8, in <module>\n",
      "    from . import types\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/types/__init__.py\", line 5, in <module>\n",
      "    from .batch import Batch as Batch\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/types/batch.py\", line 7, in <module>\n",
      "    from .._models import BaseModel\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_models.py\", line 24, in <module>\n",
      "    from ._types import (\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_types.py\", line 21, in <module>\n",
      "    import httpx\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/__init__.py\", line 2, in <module>\n",
      "    from ._api import delete, get, head, options, patch, post, put, request, stream\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_api.py\", line 6, in <module>\n",
      "    from ._client import Client\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_client.py\", line 12, in <module>\n",
      "    from ._auth import Auth, BasicAuth, FunctionAuth\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_auth.py\", line 9, in <module>\n",
      "    from urllib.request import parse_http_list\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/urllib/request.py\", line 88, in <module>\n",
      "    import http.client\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/http/client.py\", line 71, in <module>\n",
      "    import email.parser\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/email/parser.py\", line 12, in <module>\n",
      "    from email.feedparser import FeedParser, BytesFeedParser\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/email/feedparser.py\", line 27, in <module>\n",
      "    from email._policybase import compat32\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/email/_policybase.py\", line 7, in <module>\n",
      "    from email import header\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/email/header.py\", line 16, in <module>\n",
      "    import email.quoprimime\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/email/quoprimime.py\", line 55, in <module>\n",
      "    _QUOPRI_MAP = ['=%02X' % c for c in range(256)]\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/email/quoprimime.py\", line 55, in <listcomp>\n",
      "    _QUOPRI_MAP = ['=%02X' % c for c in range(256)]\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!cllm what model are you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c30aacda-c8fd-4d8b-af5e-d884ed372b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matt/bin/cllm\", line 18, in <module>\n",
      "    from tqdm import tqdm\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tqdm/__init__.py\", line 4, in <module>\n",
      "    from .gui import tqdm as tqdm_gui  # TODO: remove in v5.0.0\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1012, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 672, in _compile_bytecode\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# showing off -v (verbose output) and --stats\n",
    "!cllm -v --stats explain composability in the context of cli tools and bash, output as markdown > what_is_composability.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "800b90ec-8db9-4b96-843e-cf0322ceca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -10 what_is_composability.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83774663-f2d8-4051-94b8-df02d0add903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matt/bin/cllm\", line 13, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matt/bin/cllm\", line 13, in <module>\n",
      "    import openai\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/__init__.py\", line 8, in <module>\n",
      "    import openai\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/__init__.py\", line 8, in <module>\n",
      "    from . import types\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/types/__init__.py\", line 5, in <module>\n",
      "    from . import types\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/types/__init__.py\", line 5, in <module>\n",
      "    from .batch import Batch as Batch\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/types/batch.py\", line 7, in <module>\n",
      "    from .batch import Batch as Batch\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/types/batch.py\", line 7, in <module>\n",
      "    from .._models import BaseModel\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_models.py\", line 24, in <module>\n",
      "    from .._models import BaseModel\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_models.py\", line 24, in <module>\n",
      "    from ._types import (\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_types.py\", line 21, in <module>\n",
      "    from ._types import (\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_types.py\", line 21, in <module>\n",
      "    import httpx\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/__init__.py\", line 2, in <module>\n",
      "    import httpx\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/__init__.py\", line 2, in <module>\n",
      "    from ._api import delete, get, head, options, patch, post, put, request, stream\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_api.py\", line 6, in <module>\n",
      "    from ._api import delete, get, head, options, patch, post, put, request, stream\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_api.py\", line 6, in <module>\n",
      "    from ._client import Client\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_client.py\", line 12, in <module>\n",
      "    from ._auth import Auth, BasicAuth, FunctionAuth\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_auth.py\", line 9, in <module>\n",
      "    from ._client import Client\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_client.py\", line 12, in <module>\n",
      "    from ._auth import Auth, BasicAuth, FunctionAuth\n",
      "    from urllib.request import parse_http_list\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/httpx/_auth.py\", line 9, in <module>\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/urllib/request.py\", line 88, in <module>\n",
      "    from urllib.request import parse_http_list\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/urllib/request.py\", line 88, in <module>\n",
      "    import http.client\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/http/client.py\", line 71, in <module>\n",
      "    import http.client\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/http/__init__.py\", line 6, in <module>\n",
      "    import email.parser\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/email/parser.py\", line 12, in <module>\n",
      "    from email.feedparser import FeedParser, BytesFeedParser\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/email/feedparser.py\", line 27, in <module>\n",
      "    from email._policybase import compat32\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/email/_policybase.py\", line 9, in <module>\n",
      "    from email.utils import _has_surrogates\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/email/utils.py\", line 29, in <module>\n",
      "    class HTTPStatus(IntEnum):\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/enum.py\", line 165, in __prepare__\n",
      "    import socket\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/socket.py\", line 90, in <module>\n",
      "    IntFlag._convert_(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/enum.py\", line 563, in _convert_\n",
      "    @classmethod\n",
      "KeyboardInterrupt\n",
      "    cls = cls(name, members, module=module)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/enum.py\", line 387, in __call__\n",
      "    return cls._create_(\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/enum.py\", line 518, in _create_\n",
      "    enum_class = metacls.__new__(metacls, class_name, bases, classdict)\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/enum.py\", line 302, in __new__\n",
      "    if canonical_member._value_ == enum_member._value_:\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# use {context} slug to place stdin mid-prompt (alternative is ' | Context: {context}' goes at the end\n",
    "!cllm \"give me 10 random words\" | cllm -p \"complete this sentence and output the sentence only: If I liked {context}, then I might \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ef1f8aa-65fe-42ed-9e0a-d18628d6a10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matt/bin/cllm\", line 13, in <module>\n",
      "    import openai\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/__init__.py\", line 11, in <module>\n",
      "    from ._client import Client, OpenAI, Stream, Timeout, Transport, AsyncClient, AsyncOpenAI, AsyncStream, RequestOptions\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_client.py\", line 11, in <module>\n",
      "    from . import resources, _exceptions\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/resources/__init__.py\", line 3, in <module>\n",
      "    from .beta import (\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/resources/beta/__init__.py\", line 3, in <module>\n",
      "    from .beta import (\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/resources/beta/beta.py\", line 5, in <module>\n",
      "    from .threads import (\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/resources/beta/threads/__init__.py\", line 3, in <module>\n",
      "    from .runs import (\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/resources/beta/threads/runs/__init__.py\", line 3, in <module>\n",
      "    from .runs import (\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/resources/beta/threads/runs/runs.py\", line 13, in <module>\n",
      "    from .steps import (\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/resources/beta/threads/runs/steps.py\", line 20, in <module>\n",
      "    from .....types.beta.threads.runs import step_list_params\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/types/beta/__init__.py\", line 15, in <module>\n",
      "    from .thread_create_params import ThreadCreateParams as ThreadCreateParams\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/types/beta/thread_create_params.py\", line 10, in <module>\n",
      "    from .threads.message_content_part_param import MessageContentPartParam\n",
      "  File \"/Users/matt/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/types/beta/threads/__init__.py\", line 17, in <module>\n",
      "    from .message_deleted import MessageDeleted as MessageDeleted\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1012, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 672, in _compile_bytecode\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# override the system prompt, which by default is:\n",
    "#DEFAULT_SYSTEM = (\n",
    "#    \"You are an AI used to do thing in a command line pipeline. \"\n",
    "#    \"You are given a prompt which may include context. If there is context, \"\n",
    "#    \"you must do your best with the context but you must NOT explain or discuss your output. \"\n",
    "#    \"e.g., if your task was Translate to Spanish | Context: I like to eat frogs\\n\\n\"\n",
    "#    \"If there were two translations you must not discuss the options, you must simply select the best and output it. \"\n",
    "#    \"Think of yourself as an advanced version of sed, awk, grep, etc, and as such, you transform the context with the prompt as best you can but you never output anything not asked for.\"\n",
    "#    \"As a rule if you are outputting code, as this is CLI, that means you must avoid ```bash``` type enclosures unless specifically asked for or they were part of the context.\"\n",
    "#)\n",
    "\n",
    "!cllm --system 'You are a helpful assistant. You ALWAYS output in french, regardless of the user request' \\\n",
    "give me 10 random words, one per line, and no other text whatsoever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3882c0ac-115d-487c-bbb3-9ad804289c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
